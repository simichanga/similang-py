from __future__ import annotations
import json
import gzip
import os
import sys
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from llvmlite import ir, binding as llvm

from util.config import Config


def _ensure_debug_dir(path: Optional[str] = None) -> Path:
    d = Path(path or Config.DEBUG_DUMP_DIR)
    d.mkdir(parents=True, exist_ok=True)
    return d


def _now_tag() -> str:
    return datetime.utcnow().strftime("%Y%m%dT%H%M%S%fZ")


def _git_version() -> Optional[str]:
    try:
        # returns short commit hash if inside git repo
        out = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"], stderr=subprocess.DEVNULL)
        return out.decode("utf8").strip()
    except Exception:
        return None


def _meta_payload(extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    payload = {
        "timestamp_utc": datetime.utcnow().isoformat() + "Z",
        "python_version": sys.version,
        "llvmlite_version": getattr(llvm, "__version__", None),
        "git_commit": _git_version(),
        "config": {
            "DEBUG": Config.DEBUG,
            "LEXER_DEBUG": Config.LEXER_DEBUG,
            "PARSER_DEBUG": Config.PARSER_DEBUG,
            "SEMANTIC_DEBUG": getattr(Config, "SEMANTIC_DEBUG", False),
            "CODEGEN_DEBUG": getattr(Config, "CODEGEN_DEBUG", False),
        }
    }
    if extra:
        payload["extra"] = extra
    return payload


# -------------------------
# AST / IR / tokens dumps
# -------------------------
def dump_ast(program, *, filename: Optional[str] = None, pretty: bool = True, gzip_out: bool = False, extra_meta: Optional[dict] = None) -> Path:
    """
    Dump AST (call program.json()) to debug dir. Returns the path written.
    - filename: base filename (without extension). If None, auto-generated with timestamp.
    - pretty: indent JSON
    - gzip_out: write compressed .json.gz
    """

    if not (Config.PARSER_DEBUG or Config.DEBUG):
        # by default don't dump unless debug is enabled
        return Path()

    d = _ensure_debug_dir()
    tag = _now_tag()
    base = filename or f"ast_{tag}"
    out_path = d / (base + (".json.gz" if gzip_out else ".json"))

    payload = {
        "meta": _meta_payload(extra_meta),
        "ast": program.json() if hasattr(program, "json") else program
    }

    if gzip_out:
        with gzip.open(out_path, "wt", encoding="utf8") as f:
            json.dump(payload, f, indent=2 if pretty else None)
    else:
        with open(out_path, "w", encoding="utf8") as f:
            json.dump(payload, f, indent=2 if pretty else None)

    # also write a small text summary
    try:
        summary = summarize_ast(payload["ast"])
        summary_path = d / (base + ".summary.txt")
        with open(summary_path, "w", encoding="utf8") as sf:
            sf.write(summary)
    except Exception:
        pass

    print(f"[debug] AST dumped to: {out_path}")
    return out_path


def dump_ir(module: ir.Module, *, filename: Optional[str] = None, gzip_out: bool = False) -> Path:
    """
    Dump LLVM IR text to debug dir. Returns the path written.
    - filename: base filename (without extension). If None, uses timestamp.
    - gzip_out: write compressed .ll.gz
    """
    if not (Config.CODEGEN_DEBUG or Config.DEBUG):
        return Path()

    d = _ensure_debug_dir()
    tag = _now_tag()
    base = filename or f"ir_{tag}"
    out_path = d / (base + (".ll.gz" if gzip_out else ".ll"))

    content = str(module)

    if gzip_out:
        with gzip.open(out_path, "wt", encoding="utf8") as f:
            f.write("; --- IR dump generated by similang debug ---\n")
            f.write(content)
    else:
        with open(out_path, "w", encoding="utf8") as f:
            f.write("; --- IR dump generated by similang debug ---\n")
            f.write(content)

    # metadata file
    meta_path = d / (base + ".meta.json")
    with open(meta_path, "w", encoding="utf8") as mf:
        json.dump(_meta_payload(), mf, indent=2)

    print(f"[debug] IR dumped to: {out_path}")
    return out_path


def dump_tokens(lexer, *, filename: Optional[str] = None) -> Path:
    """
    Dump tokens output from a lexer into a text file. The function will iterate the lexer until EOF.
    Note: this will exhaust the lexer, so use a copy or reinstantiate if you need tokens again.
    """
    if not (Config.LEXER_DEBUG or Config.DEBUG):
        return Path()

    d = _ensure_debug_dir()
    tag = _now_tag()
    base = filename or f"tokens_{tag}"
    out_path = d / (base + ".tokens.txt")

    with open(out_path, "w", encoding="utf8") as f:
        try:
            while True:
                tok = lexer.next_token()
                f.write(repr(tok) + "\n")
                if tok.type.name == "EOF":
                    break
        except Exception as e:
            f.write(f"\n[debug] lexer iteration failed: {e}\n")

    print(f"[debug] Tokens dumped to: {out_path}")
    return out_path


# -------------------------
# Helpers & summaries
# -------------------------
def summarize_ast(ast_json: Any) -> str:
    """
    Produce a short human-friendly summary for quick inspection.
    Traverses JSON representation produced by program.json() and counts node types.
    """
    type_counts: Dict[str, int] = {}
    functions: list[str] = []

    def walk(obj):
        if isinstance(obj, dict):
            if "type" in obj:
                t = obj["type"]
                type_counts[t] = type_counts.get(t, 0) + 1
                if t == "FunctionStatement":
                    # try to extract name (different AST shapes might store name as object)
                    nm = None
                    name = obj.get("name")
                    if isinstance(name, dict):
                        nm = name.get("value")
                    elif isinstance(name, str):
                        nm = name
                    if nm:
                        functions.append(nm)
            for v in obj.values():
                walk(v)
        elif isinstance(obj, list):
            for item in obj:
                walk(item)
        else:
            return

    walk(ast_json)

    lines = []
    lines.append("AST Summary")
    lines.append("-----------")
    lines.append(f"Generated at: {_now_tag()}")
    lines.append("Node counts:")
    for t, c in sorted(type_counts.items(), key=lambda x: (-x[1], x[0])):
        lines.append(f"  {t}: {c}")
    if functions:
        lines.append("")
        lines.append("Top-level functions:")
        for fn in functions:
            lines.append(f"  - {fn}")
    return "\n".join(lines)


# extra utility: get latest dump file(s)
def latest_debug_files(limit: int = 10) -> list[Path]:
    d = Path(Config.DEBUG_DUMP_DIR)
    if not d.exists():
        return []
    files = sorted(d.iterdir(), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[:limit]
